{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9624f9-a50c-4614-b8d4-8edcc2af4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018a710-fc5c-4aa2-aa77-464a0e06c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3816e597-2636-4866-af20-f77e2fff1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "order_payments = pd.read_csv('data/olist_order_payments_dataset.csv')\n",
    "order_reviews = pd.read_csv('data/olist_order_reviews_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "sellers = pd.read_csv('data/olist_sellers_dataset.csv')\n",
    "customers = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "geolocation = pd.read_csv('data/olist_geolocation_dataset.csv')\n",
    "products_translation = pd.read_csv('data/product_category_name_translation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1062c7-00d8-478b-9271-cdfbd9765774",
   "metadata": {},
   "source": [
    "# 1. Data Exploration and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df046bd4-530f-45c0-90b6-44ac1a34d2ce",
   "metadata": {},
   "source": [
    "## 1.1. Dropping columns that would not influence the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389960b-a2ed-428b-8c98-d7e7608dd4bc",
   "metadata": {},
   "source": [
    "## - orders Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13125be5-7faa-449b-a5ae-dae640e81d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_new = orders.drop(['order_approved_at', 'order_purchase_timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bd8ef-b10d-4fac-a6bc-3ee131b278a9",
   "metadata": {},
   "source": [
    "## - order_items exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd445a9-20cb-439b-8817-11afc92aba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_new = order_items.drop(['seller_id', 'shipping_limit_date', 'freight_value'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bfc05-f901-4d99-929d-bc45fc216e03",
   "metadata": {},
   "source": [
    "## - order_payments exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73103a5-041d-4b13-b6f7-a6e498085156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103886 entries, 0 to 103885\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   order_id              103886 non-null  object \n",
      " 1   payment_sequential    103886 non-null  int64  \n",
      " 2   payment_type          103886 non-null  object \n",
      " 3   payment_installments  103886 non-null  int64  \n",
      " 4   payment_value         103886 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "order_payments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1716f-dd97-479e-818a-adb8cfacd04b",
   "metadata": {},
   "source": [
    "###### Drop this dataset because the information is not required for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d084b-813e-4fcc-89a6-2b0951acc4e3",
   "metadata": {},
   "source": [
    "## - order_reviews exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab4b8f7-fe26-4e7f-94f7-17c42e1b6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_reviews_new = order_reviews.drop(['review_comment_title' , 'review_comment_message' , 'review_creation_date' , 'review_answer_timestamp'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9f3db-51ce-496b-b9f9-038a0eb62645",
   "metadata": {},
   "source": [
    "## - products exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80dbf846-08f3-4f63-881d-e21bb2457c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new = products.drop(['product_name_lenght', 'product_description_lenght', 'product_weight_g'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "751439ba-72b4-4169-93ef-04b0ea59bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the products DataFrame with the products_translation DataFrame\n",
    "products_merged = products_new.merge(products_translation, \n",
    "                                 left_on='product_category_name', \n",
    "                                 right_on='product_category_name', \n",
    "                                 how='left')\n",
    "\n",
    "# Drop the original 'product_category_name' column\n",
    "products_merged.drop('product_category_name', axis=1, inplace=True)\n",
    "\n",
    "# Optionally, rename the 'product_category_name_english' column to 'product_category_name'\n",
    "products_merged.rename(columns={'product_category_name_english': 'product_category_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158b98f-740c-45ab-9c70-3329dfca6672",
   "metadata": {},
   "source": [
    "## - sellers exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a2d6e4-6ece-4dea-83d1-6b78cfd9581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_new = sellers.drop(['seller_zip_code_prefix'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10259666-2eda-43b2-b288-d7d32d433fb2",
   "metadata": {},
   "source": [
    "## - customers exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e9a83a-e8da-4b2d-bf3c-08a8fbd11e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_new = customers.drop(['customer_unique_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a198aa4-4d8c-4b5d-b7fa-7d3df9d482b9",
   "metadata": {},
   "source": [
    "## - geolocation exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce4e78c-2f5e-4a88-b2cb-2582bbe465a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation = geolocation.groupby(['geolocation_city','geolocation_state','geolocation_zip_code_prefix']).last()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a1fd7f-e2fc-44b1-aae7-7eac5e9a0b9a",
   "metadata": {},
   "source": [
    "# 1.2. Merging of datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73271edd-d1ef-4f0b-9e1d-68de15ed06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets related to orders\n",
    "orders_forecasting = orders \\\n",
    "    .merge(order_items_new, on='order_id', how='left') \\\n",
    "    .merge(order_reviews_new, on='order_id', how='left') \\\n",
    "    .merge(products_merged, on='product_id', how='left') \\\n",
    "    .merge(customers_new, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fb9ae6-d446-4312-b03f-2e379c4e9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114092 entries, 0 to 114091\n",
      "Data columns (total 21 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   order_id                       114092 non-null  object \n",
      " 1   customer_id                    114092 non-null  object \n",
      " 2   order_status                   114092 non-null  object \n",
      " 3   order_purchase_timestamp       114092 non-null  object \n",
      " 4   order_approved_at              113930 non-null  object \n",
      " 5   order_delivered_carrier_date   112112 non-null  object \n",
      " 6   order_delivered_customer_date  110839 non-null  object \n",
      " 7   order_estimated_delivery_date  114092 non-null  object \n",
      " 8   order_item_id                  113314 non-null  float64\n",
      " 9   product_id                     113314 non-null  object \n",
      " 10  price                          113314 non-null  float64\n",
      " 11  review_id                      113131 non-null  object \n",
      " 12  review_score                   113131 non-null  float64\n",
      " 13  product_photos_qty             111702 non-null  float64\n",
      " 14  product_length_cm              113296 non-null  float64\n",
      " 15  product_height_cm              113296 non-null  float64\n",
      " 16  product_width_cm               113296 non-null  float64\n",
      " 17  product_category_name          111678 non-null  object \n",
      " 18  customer_zip_code_prefix       114092 non-null  int64  \n",
      " 19  customer_city                  114092 non-null  object \n",
      " 20  customer_state                 114092 non-null  object \n",
      "dtypes: float64(7), int64(1), object(13)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "orders_forecasting.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d55aa66-e330-4185-9c2f-8e1aa7fc6b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>...</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>product_photos_qty</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>...</td>\n",
       "      <td>a54f0611adc9ed256b57ede6b6eb5114</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>housewares</td>\n",
       "      <td>3149</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>...</td>\n",
       "      <td>8d5266042046a06655c8db133d120ba5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>perfumery</td>\n",
       "      <td>47813</td>\n",
       "      <td>barreiras</td>\n",
       "      <td>BA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>...</td>\n",
       "      <td>e73b67b67587f7644d5bd1a52deb1b01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>auto</td>\n",
       "      <td>75265</td>\n",
       "      <td>vianopolis</td>\n",
       "      <td>GO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949d5b44dbf5de918fe9c16f97b45f8a</td>\n",
       "      <td>f88197465ea7920adcdbec7375364d82</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-11-18 19:28:06</td>\n",
       "      <td>2017-11-18 19:45:59</td>\n",
       "      <td>2017-11-22 13:39:59</td>\n",
       "      <td>2017-12-02 00:28:42</td>\n",
       "      <td>2017-12-15 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>d0b61bfb1de832b15ba9d266ca96e5b0</td>\n",
       "      <td>...</td>\n",
       "      <td>359d03e676b3c069f62cadba8dd3f6e8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>pet_shop</td>\n",
       "      <td>59296</td>\n",
       "      <td>sao goncalo do amarante</td>\n",
       "      <td>RN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ad21c59c0840e6cb83a9ceb5573f8159</td>\n",
       "      <td>8ab97904e6daea8866dbdbc4fb7aad2c</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-13 21:18:39</td>\n",
       "      <td>2018-02-13 22:20:29</td>\n",
       "      <td>2018-02-14 19:46:34</td>\n",
       "      <td>2018-02-16 18:17:02</td>\n",
       "      <td>2018-02-26 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65266b2da20d04dbe00c5c2d3bb7859e</td>\n",
       "      <td>...</td>\n",
       "      <td>e50934924e227544ba8246aeb3770dd4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>stationery</td>\n",
       "      <td>9195</td>\n",
       "      <td>santo andre</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
       "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
       "\n",
       "  order_status order_purchase_timestamp    order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
       "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
       "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
       "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
       "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
       "\n",
       "  order_estimated_delivery_date  order_item_id  \\\n",
       "0           2017-10-18 00:00:00            1.0   \n",
       "1           2018-08-13 00:00:00            1.0   \n",
       "2           2018-09-04 00:00:00            1.0   \n",
       "3           2017-12-15 00:00:00            1.0   \n",
       "4           2018-02-26 00:00:00            1.0   \n",
       "\n",
       "                         product_id  ...                         review_id  \\\n",
       "0  87285b34884572647811a353c7ac498a  ...  a54f0611adc9ed256b57ede6b6eb5114   \n",
       "1  595fac2a385ac33a80bd5114aec74eb8  ...  8d5266042046a06655c8db133d120ba5   \n",
       "2  aa4383b373c6aca5d8797843e5594415  ...  e73b67b67587f7644d5bd1a52deb1b01   \n",
       "3  d0b61bfb1de832b15ba9d266ca96e5b0  ...  359d03e676b3c069f62cadba8dd3f6e8   \n",
       "4  65266b2da20d04dbe00c5c2d3bb7859e  ...  e50934924e227544ba8246aeb3770dd4   \n",
       "\n",
       "  review_score  product_photos_qty  product_length_cm  product_height_cm  \\\n",
       "0          4.0                 4.0               19.0                8.0   \n",
       "1          4.0                 1.0               19.0               13.0   \n",
       "2          5.0                 1.0               24.0               19.0   \n",
       "3          5.0                 3.0               30.0               10.0   \n",
       "4          5.0                 4.0               51.0               15.0   \n",
       "\n",
       "   product_width_cm  product_category_name customer_zip_code_prefix  \\\n",
       "0              13.0             housewares                     3149   \n",
       "1              19.0              perfumery                    47813   \n",
       "2              21.0                   auto                    75265   \n",
       "3              20.0               pet_shop                    59296   \n",
       "4              15.0             stationery                     9195   \n",
       "\n",
       "             customer_city customer_state  \n",
       "0                sao paulo             SP  \n",
       "1                barreiras             BA  \n",
       "2               vianopolis             GO  \n",
       "3  sao goncalo do amarante             RN  \n",
       "4              santo andre             SP  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_forecasting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4124cca-5165-4869-90cb-3a8e0fd82938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mindy\\AppData\\Local\\Temp\\ipykernel_3900\\3540856820.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  route_optimisation.drop_duplicates(inplace=True)\n",
      "C:\\Users\\Mindy\\AppData\\Local\\Temp\\ipykernel_3900\\3540856820.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  route_optimisation['order_delivered_carrier_date'] = route_optimisation['order_delivered_carrier_date'].str[:10]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>geolocation_lat</th>\n",
       "      <th>geolocation_lng</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3149</td>\n",
       "      <td>-23.575377</td>\n",
       "      <td>-46.587410</td>\n",
       "      <td>2017-10-04</td>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>sao paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3149</td>\n",
       "      <td>-23.583452</td>\n",
       "      <td>-46.586284</td>\n",
       "      <td>2017-10-04</td>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>sao paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47813</td>\n",
       "      <td>-12.124719</td>\n",
       "      <td>-45.011148</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>barreiras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75265</td>\n",
       "      <td>-16.743570</td>\n",
       "      <td>-48.511633</td>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>vianopolis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75265</td>\n",
       "      <td>-16.746170</td>\n",
       "      <td>-48.522116</td>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>vianopolis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_zip_code_prefix  geolocation_lat  geolocation_lng  \\\n",
       "0                      3149       -23.575377       -46.587410   \n",
       "1                      3149       -23.583452       -46.586284   \n",
       "2                     47813       -12.124719       -45.011148   \n",
       "3                     75265       -16.743570       -48.511633   \n",
       "4                     75265       -16.746170       -48.522116   \n",
       "\n",
       "  order_delivered_carrier_date                          order_id customer_city  \n",
       "0                   2017-10-04  e481f51cbdc54678b7cc49136f2d6af7     sao paulo  \n",
       "1                   2017-10-04  e481f51cbdc54678b7cc49136f2d6af7     sao paulo  \n",
       "2                   2018-07-26  53cdb2fc8bc7dce0b6741e2150273451     barreiras  \n",
       "3                   2018-08-08  47770eb9100c2d0c44946d9cf07ec65d    vianopolis  \n",
       "4                   2018-08-08  47770eb9100c2d0c44946d9cf07ec65d    vianopolis  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_order_details = pd.merge(geolocation, orders_forecasting, left_on='geolocation_zip_code_prefix', right_on='customer_zip_code_prefix', how='right')\n",
    "route_optimisation = full_order_details[['customer_zip_code_prefix','geolocation_lat','geolocation_lng','order_delivered_carrier_date','order_id','customer_city']]\n",
    "route_optimisation.drop_duplicates(inplace=True)\n",
    "route_optimisation['order_delivered_carrier_date'] = route_optimisation['order_delivered_carrier_date'].str[:10]\n",
    "route_optimisation.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9046315-1a9f-423d-88ea-91ccd5fee15b",
   "metadata": {},
   "source": [
    "# 2. Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5477f-b393-4de4-b9e9-64c3b60c0a61",
   "metadata": {},
   "source": [
    "## 2.1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b20e83c-bfa8-4397-831a-e78bceeaad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orders_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab8f697-f50c-4ec0-9c0f-7c20cca9d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mindy\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 1.7931732579606117\n"
     ]
    }
   ],
   "source": [
    "# Preprocess datetime columns: Convert to pandas datetime type\n",
    "datetime_features = ['order_purchase_timestamp', 'order_approved_at',\n",
    "                     'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "                     'order_estimated_delivery_date']\n",
    "for feature in datetime_features:\n",
    "    df[feature] = pd.to_datetime(df[feature])\n",
    "\n",
    "# Drop rows with NaN values in the target variable\n",
    "df = df.dropna(subset=['review_score'])\n",
    "\n",
    "# Assuming 'review_score' is the target and the rest are features\n",
    "X = df.drop('review_score', axis=1)\n",
    "y = df['review_score']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Preprocessing pipelines for both numerical and categorical data\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features.remove('product_photos_qty')  # Assuming this is a count and does not need scaling\n",
    "categorical_features = ['order_status', 'product_category_name']  # Assuming these are the categorical features\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputation with mean for numeric columns\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))  # One-hot encode and ensure dense output\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Preprocess the training and test data\n",
    "X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "X_test_prepared = preprocessor.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict and evaluate the Random Forest model\n",
    "rf_pred = rf_model.predict(X_test_prepared)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "print(f\"Random Forest MSE: {rf_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3410bb-1641-4e64-9f36-48325c97b0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     feature  importance\n",
      "5   customer_zip_code_prefix    0.300528\n",
      "1                      price    0.184720\n",
      "3          product_height_cm    0.096482\n",
      "2          product_length_cm    0.088660\n",
      "4           product_width_cm    0.084864\n",
      "9               x0_delivered    0.075412\n",
      "0              order_item_id    0.025145\n",
      "80         x1_sports_leisure    0.008672\n",
      "57          x1_health_beauty    0.008069\n",
      "63             x1_housewares    0.007619\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances from the model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Assuming preprocessor is your ColumnTransformer with pipelines for 'num' and 'cat' preprocessing\n",
    "numeric_feature_names = numeric_features  # Assuming this variable is defined with your numeric feature names\n",
    "\n",
    "# Adjust retrieval of categorical feature names after one-hot encoding to accommodate your pipeline's structure\n",
    "# Note: Ensure 'cat' matches the name given in your ColumnTransformer for the categorical handling step\n",
    "categorical_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out()\n",
    "\n",
    "# Combine both lists of feature names: numerics remain unchanged, categoricals are expanded post one-hot encoding\n",
    "all_feature_names = numeric_feature_names + list(categorical_feature_names)\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Display the top 10 features ranked by importance\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a42ffbc-b299-4d4d-85df-911c0734afc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R-squared: 0.07353828386788552\n",
      "Random Forest RMSE: 1.339094193087481\n",
      "Random Forest MAE: 1.0374572056197657\n"
     ]
    }
   ],
   "source": [
    "# Random Forest predictions\n",
    "rf_pred = rf_model.predict(X_test_prepared)\n",
    "\n",
    "# Calculate R-squared for Random Forest\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "print(f\"Random Forest R-squared: {rf_r2}\")\n",
    "\n",
    "# Calculate RMSE for Random Forest\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "\n",
    "# Calculate MAE for Random Forest\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "print(f\"Random Forest MAE: {rf_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036a094-39a8-4a5f-9ceb-23a81a0444c7",
   "metadata": {},
   "source": [
    "## 2.2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "138ac545-8b52-4943-b87e-24ff6d5dc2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2829/2829 [==============================] - 16s 5ms/step - loss: 1.8879\n",
      "Epoch 2/10\n",
      "2829/2829 [==============================] - 12s 4ms/step - loss: 1.7794\n",
      "Epoch 3/10\n",
      "2829/2829 [==============================] - 11s 4ms/step - loss: 1.7684\n",
      "Epoch 4/10\n",
      "2829/2829 [==============================] - 12s 4ms/step - loss: 1.7632\n",
      "Epoch 5/10\n",
      "2829/2829 [==============================] - 11s 4ms/step - loss: 1.7567\n",
      "Epoch 6/10\n",
      "2829/2829 [==============================] - 11s 4ms/step - loss: 1.7507\n",
      "Epoch 7/10\n",
      "2829/2829 [==============================] - 14s 5ms/step - loss: 1.7453\n",
      "Epoch 8/10\n",
      "2829/2829 [==============================] - 13s 5ms/step - loss: 1.7396\n",
      "Epoch 9/10\n",
      "2829/2829 [==============================] - 13s 5ms/step - loss: 1.7353\n",
      "Epoch 10/10\n",
      "2829/2829 [==============================] - 12s 4ms/step - loss: 1.7328\n",
      "Neural Network MSE: 1.7322440147399902\n"
     ]
    }
   ],
   "source": [
    "# Define a Neural Network model\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_prepared.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the Neural Network model\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the Neural Network model\n",
    "nn_model.fit(X_train_prepared, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the Neural Network model using the test data\n",
    "nn_mse = nn_model.evaluate(X_test_prepared, y_test, verbose=0)\n",
    "print(f\"Neural Network MSE: {nn_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f8b4505-2105-47ce-8436-8eee3382d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708/708 [==============================] - 3s 3ms/step\n",
      "Neural Network R-squared: 0.10501800789124849\n",
      "Neural Network RMSE: 1.3161474186574504\n",
      "Neural Network MAE: 1.0371709433948335\n"
     ]
    }
   ],
   "source": [
    "# Neural Network predictions\n",
    "nn_pred = nn_model.predict(X_test_prepared).flatten() # Flatten if necessary\n",
    "\n",
    "# Calculate R-squared for Neural Network\n",
    "nn_r2 = r2_score(y_test, nn_pred)\n",
    "print(f\"Neural Network R-squared: {nn_r2}\")\n",
    "\n",
    "# Calculate RMSE for Neural Network\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_test, nn_pred))\n",
    "print(f\"Neural Network RMSE: {nn_rmse}\")\n",
    "\n",
    "# Calculate MAE for Neural Network\n",
    "nn_mae = mean_absolute_error(y_test, nn_pred)\n",
    "print(f\"Neural Network MAE: {nn_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ebc89-b0a7-4dba-9b68-f5dc20d25269",
   "metadata": {},
   "source": [
    "# 3. Demand Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b3435-1913-4b23-aae1-b85592aa6d5b",
   "metadata": {},
   "source": [
    "## 3.1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d432679-e770-4c23-acd8-4891e0b872bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mindy\\AppData\\Local\\Temp\\ipykernel_3900\\2508384973.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[feature] = pd.to_datetime(df[feature])\n",
      "C:\\Users\\Mindy\\AppData\\Local\\Temp\\ipykernel_3900\\2508384973.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[feature] = pd.to_datetime(df[feature])\n",
      "C:\\Users\\Mindy\\AppData\\Local\\Temp\\ipykernel_3900\\2508384973.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[feature] = pd.to_datetime(df[feature])\n",
      "C:\\Users\\Mindy\\AppData\\Local\\Temp\\ipykernel_3900\\2508384973.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[feature] = pd.to_datetime(df[feature])\n",
      "C:\\Users\\Mindy\\AppData\\Local\\Temp\\ipykernel_3900\\2508384973.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[feature] = pd.to_datetime(df[feature])\n",
      "C:\\Users\\Mindy\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "                     Feature  Importance\n",
      "21         x1_bed_bath_table    0.278054\n",
      "57          x1_health_beauty    0.165164\n",
      "53        x1_furniture_decor    0.116787\n",
      "79         x1_sports_leisure    0.114151\n",
      "29  x1_computers_accessories    0.108290\n",
      "63             x1_housewares    0.075628\n",
      "84          x1_watches_gifts    0.045261\n",
      "83                   x1_toys    0.014029\n",
      "82              x1_telephony    0.013064\n",
      "19                   x1_auto    0.012983\n",
      "\n",
      "Predicted vs Actual Orders:\n",
      "   Actual Orders  Predicted Orders\n",
      "0           2039            2039.0\n",
      "1           6943            6943.0\n",
      "2           4213            4213.0\n",
      "3           2507            2507.0\n",
      "4           9645            9645.0\n",
      "5           8331            8331.0\n",
      "6           4091            4091.0\n",
      "7          11137           11137.0\n",
      "8           6943            6943.0\n",
      "9           5950            5950.0\n",
      "\n",
      "Random Forest R-squared: 0.99999999880877\n",
      "Random Forest RMSE: 0.11578077283779242\n",
      "Random Forest MAE: 0.0020076749435665926\n"
     ]
    }
   ],
   "source": [
    "# Convert datetime columns to pandas datetime type\n",
    "datetime_features = ['order_purchase_timestamp', 'order_approved_at',\n",
    "                     'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "                     'order_estimated_delivery_date']\n",
    "for feature in datetime_features:\n",
    "    df[feature] = pd.to_datetime(df[feature])\n",
    "\n",
    "# Drop rows with NaN values in crucial columns for analysis\n",
    "df = df.dropna(subset=['product_category_name', 'price'])\n",
    "\n",
    "# Aggregate to calculate total orders per product_category_name\n",
    "df_agg = df.groupby('product_category_name').size().reset_index(name='total_orders')\n",
    "\n",
    "# Merge aggregated total orders back to the original dataframe\n",
    "df = df.merge(df_agg, on='product_category_name', how='left')\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = df.drop(['total_orders', 'order_item_id'], axis=1)  # Adjust if needed\n",
    "y = df['total_orders']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Define preprocessing pipelines for numeric and categorical data\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = ['order_status', 'product_category_name']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Preprocess the training and test data\n",
    "X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "X_test_prepared = preprocessor.transform(X_test)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=123)\n",
    "rf_model.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict on the test set and calculate evaluation metrics\n",
    "y_pred = rf_model.predict(X_test_prepared)\n",
    "rf_r2 = r2_score(y_test, y_pred)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rf_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Extract and display feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "# Handling feature names for both numeric and one-hot encoded categorical features\n",
    "try:\n",
    "    # For scikit-learn 0.22 and newer\n",
    "    categorical_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out()\n",
    "except AttributeError:\n",
    "    # Fallback for older versions of scikit-learn\n",
    "    categorical_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names(categorical_features)\n",
    "all_feature_names = numeric_features + list(categorical_feature_names)\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted numbers of orders\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual Orders': y_test.reset_index(drop=True),  # Reset index for alignment\n",
    "    'Predicted Orders': y_pred\n",
    "})\n",
    "\n",
    "# Display the feature importances and the first 10 entries of actual vs predicted orders\n",
    "print(\"Feature Importances:\")\n",
    "print(features_df.head(10))\n",
    "\n",
    "print(\"\\nPredicted vs Actual Orders:\")\n",
    "print(predictions_df.head(10))\n",
    "\n",
    "# Print model evaluation metrics\n",
    "print(f\"\\nRandom Forest R-squared: {rf_r2}\")\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "print(f\"Random Forest MAE: {rf_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf8d97-25cc-43fd-a63f-6c9dcd4f7ef2",
   "metadata": {},
   "source": [
    "## 3.2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e23e5f-31b3-43d5-b540-8b0baf3d6845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2215/2215 [==============================] - 13s 5ms/step - loss: 12716122.0000 - val_loss: 846169.5000\n",
      "Epoch 2/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 239397.7031 - val_loss: 44829.6172\n",
      "Epoch 3/100\n",
      "2215/2215 [==============================] - 9s 4ms/step - loss: 20763.8359 - val_loss: 7623.2676\n",
      "Epoch 4/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 4317.0620 - val_loss: 1873.8275\n",
      "Epoch 5/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 1095.0189 - val_loss: 684.6666\n",
      "Epoch 6/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 338.9482 - val_loss: 294.5088\n",
      "Epoch 7/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 138.1505 - val_loss: 196.7446\n",
      "Epoch 8/100\n",
      "2215/2215 [==============================] - 12s 5ms/step - loss: 84.2305 - val_loss: 149.7566\n",
      "Epoch 9/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 63.5699 - val_loss: 119.7059\n",
      "Epoch 10/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 61.1607 - val_loss: 105.8418\n",
      "Epoch 11/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 44.3918 - val_loss: 84.9694\n",
      "Epoch 12/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 35.7752 - val_loss: 71.1973\n",
      "Epoch 13/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 37.2727 - val_loss: 58.2350\n",
      "Epoch 14/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 30.6835 - val_loss: 193.2534\n",
      "Epoch 15/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 27.5610 - val_loss: 32.4966\n",
      "Epoch 16/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 21.4577 - val_loss: 30.9740\n",
      "Epoch 17/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 30.8711 - val_loss: 16.4952\n",
      "Epoch 18/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 18.1312 - val_loss: 28.1283\n",
      "Epoch 19/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 16.5160 - val_loss: 12.2554\n",
      "Epoch 20/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 22.4486 - val_loss: 31.9409\n",
      "Epoch 21/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 19.9468 - val_loss: 7.5136\n",
      "Epoch 22/100\n",
      "2215/2215 [==============================] - 12s 5ms/step - loss: 13.9022 - val_loss: 26.3235\n",
      "Epoch 23/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 16.8350 - val_loss: 10.4008\n",
      "Epoch 24/100\n",
      "2215/2215 [==============================] - 9s 4ms/step - loss: 15.8290 - val_loss: 5.0132\n",
      "Epoch 25/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 24.5784 - val_loss: 1.6034\n",
      "Epoch 26/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 9.5511 - val_loss: 3.8136\n",
      "Epoch 27/100\n",
      "2215/2215 [==============================] - 12s 5ms/step - loss: 15.5739 - val_loss: 4.9340\n",
      "Epoch 28/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 15.5371 - val_loss: 2.7755\n",
      "Epoch 29/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 15.8829 - val_loss: 1.1572\n",
      "Epoch 30/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 13.9540 - val_loss: 22.0151\n",
      "Epoch 31/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 19.9482 - val_loss: 38.4117\n",
      "Epoch 32/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 13.6531 - val_loss: 1.8522\n",
      "Epoch 33/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 13.6264 - val_loss: 1.4408\n",
      "Epoch 34/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 30.7759 - val_loss: 2.5455\n",
      "Epoch 35/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 7.5187 - val_loss: 9.0613\n",
      "Epoch 36/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 22.0196 - val_loss: 1.5309\n",
      "Epoch 37/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 13.2213 - val_loss: 2.9602\n",
      "Epoch 38/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 10.0236 - val_loss: 200.9959\n",
      "Epoch 39/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 17.0260 - val_loss: 21.3138\n",
      "Epoch 40/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 15.7280 - val_loss: 17.7120\n",
      "Epoch 41/100\n",
      "2215/2215 [==============================] - 18s 8ms/step - loss: 16.1628 - val_loss: 23.6135\n",
      "Epoch 42/100\n",
      "2215/2215 [==============================] - 15s 7ms/step - loss: 17.4534 - val_loss: 214.9461\n",
      "Epoch 43/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 12.2700 - val_loss: 3.8290\n",
      "Epoch 44/100\n",
      "2215/2215 [==============================] - 13s 6ms/step - loss: 14.3354 - val_loss: 6.0624\n",
      "Epoch 45/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 15.7782 - val_loss: 4.3272\n",
      "Epoch 46/100\n",
      "2215/2215 [==============================] - 9s 4ms/step - loss: 11.8735 - val_loss: 1.1045\n",
      "Epoch 47/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 15.1809 - val_loss: 3.6431\n",
      "Epoch 48/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 18.9768 - val_loss: 0.8511\n",
      "Epoch 49/100\n",
      "2215/2215 [==============================] - 9s 4ms/step - loss: 16.6639 - val_loss: 5.6747\n",
      "Epoch 50/100\n",
      "2215/2215 [==============================] - 9s 4ms/step - loss: 10.0139 - val_loss: 29.8656\n",
      "Epoch 51/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 14.7355 - val_loss: 10.4625\n",
      "Epoch 52/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 19.7373 - val_loss: 1.6188\n",
      "Epoch 53/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 14.7758 - val_loss: 1.9005\n",
      "Epoch 54/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 23.0135 - val_loss: 1.4756\n",
      "Epoch 55/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 12.5021 - val_loss: 18.6706\n",
      "Epoch 56/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 17.2436 - val_loss: 8.5349\n",
      "Epoch 57/100\n",
      "2215/2215 [==============================] - 9s 4ms/step - loss: 17.7441 - val_loss: 1.8910\n",
      "Epoch 58/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 14.0701 - val_loss: 1.6396\n",
      "Epoch 59/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 17.7067 - val_loss: 1.7808\n",
      "Epoch 60/100\n",
      "2215/2215 [==============================] - 9s 4ms/step - loss: 13.8836 - val_loss: 16.0210\n",
      "Epoch 61/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 12.0294 - val_loss: 28.9003\n",
      "Epoch 62/100\n",
      "2215/2215 [==============================] - 10s 4ms/step - loss: 25.0474 - val_loss: 0.8387\n",
      "Epoch 63/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 7.7187 - val_loss: 19.8031\n",
      "Epoch 64/100\n",
      "2215/2215 [==============================] - 11s 5ms/step - loss: 12.6964 - val_loss: 1.7970\n",
      "Epoch 65/100\n",
      "2215/2215 [==============================] - 10s 5ms/step - loss: 21.7310 - val_loss: 27.9346\n",
      "Epoch 66/100\n",
      "2202/2215 [============================>.] - ETA: 0s - loss: 9.8401"
     ]
    }
   ],
   "source": [
    "# Neural Network Model Training\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_prepared.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "nn_history = nn_model.fit(X_train_prepared, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predict on the test set with the Neural Network\n",
    "y_pred_nn = nn_model.predict(X_test_prepared).flatten()\n",
    "\n",
    "# Evaluation metrics for the Neural Network\n",
    "nn_r2 = r2_score(y_test, y_pred_nn)\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "nn_mae = mean_absolute_error(y_test, y_pred_nn)\n",
    "\n",
    "# Function to calculate permutation feature importance\n",
    "def calculate_permutation_importance(model, X_test_prepared, y_test, feature_names):\n",
    "    baseline_mse = mean_squared_error(y_test, model.predict(X_test_prepared))\n",
    "    importance_scores = []\n",
    "\n",
    "    for i, name in enumerate(feature_names):\n",
    "        X_test_shuffled = X_test_prepared.copy()\n",
    "        X_test_shuffled[:, i] = shuffle(X_test_shuffled[:, i])\n",
    "\n",
    "        shuffled_mse = mean_squared_error(y_test, model.predict(X_test_shuffled))\n",
    "        importance = shuffled_mse - baseline_mse\n",
    "        importance_scores.append((name, importance))\n",
    "\n",
    "    importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return importance_scores\n",
    "\n",
    "# Get all feature names from the preprocessing pipeline\n",
    "numeric_feature_names = numeric_features\n",
    "categories = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "categorical_feature_names = [f\"{cat}\" for cat in categories]\n",
    "all_feature_names = numeric_feature_names + list(categorical_feature_names)\n",
    "\n",
    "# Create and display a DataFrame for Neural Network predictions vs actual orders\n",
    "nn_predictions_df = pd.DataFrame({\n",
    "    'Actual Orders': y_test.reset_index(drop=True),\n",
    "    'Predicted Orders': y_pred_nn\n",
    "})\n",
    "\n",
    "# Calculate permutation feature importance for the Neural Network\n",
    "nn_feature_importance = calculate_permutation_importance(nn_model, X_test_prepared, y_test, all_feature_names)\n",
    "\n",
    "# Display the top 10 most important features\n",
    "print(\"Top 10 Feature Importances from Neural Network:\")\n",
    "for feature, importance in nn_feature_importance[:10]:\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "print(\"\\nNeural Network Predicted vs Actual Orders:\")\n",
    "print(nn_predictions_df.head(10))\n",
    "\n",
    "print(f\"Neural Network R-squared: {nn_r2}\")\n",
    "print(f\"Neural Network RMSE: {nn_rmse}\")\n",
    "print(f\"Neural Network MAE: {nn_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ba595-5ea8-48a2-838c-9b483732c852",
   "metadata": {},
   "source": [
    "## 3.3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03787f0f-a997-4de5-8ed0-50fb3b8408a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prepared training and test datasets into DMatrix objects, optimized for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_prepared, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_prepared, label=y_test)\n",
    "\n",
    "# Define XGBoost model parameters\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Specify the number of boosting rounds\n",
    "num_boost_round = 100\n",
    "\n",
    "# Train the XGBoost model\n",
    "bst = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "# Predict on the test set with XGBoost\n",
    "y_pred_xgb = bst.predict(dtest)\n",
    "\n",
    "# Evaluation metrics for the XGBoost model\n",
    "xgb_r2 = r2_score(y_test, y_pred_xgb)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "xgb_mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "# Get feature importance and display\n",
    "importance_weight = bst.get_score(importance_type='weight')\n",
    "importance_gain = bst.get_score(importance_type='gain')\n",
    "\n",
    "importance_weight_df = pd.DataFrame(list(importance_weight.items()), columns=['Feature', 'Score']).sort_values(by='Score', ascending=False)\n",
    "importance_gain_df = pd.DataFrame(list(importance_gain.items()), columns=['Feature', 'Score']).sort_values(by='Score', ascending=False)\n",
    "\n",
    "print(\"\\nXGBoost Feature Importance based on weight:\")\n",
    "print(importance_weight_df.head(10))\n",
    "\n",
    "print(\"\\nXGBoost Feature Importance based on gain:\")\n",
    "print(importance_gain_df.head(10))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"\\nXGBoost R-squared: {xgb_r2}\")\n",
    "print(f\"XGBoost RMSE: {xgb_rmse}\")\n",
    "print(f\"XGBoost MAE: {xgb_mae}\")\n",
    "\n",
    "# Optionally, create and display a DataFrame for XGBoost predictions vs actual orders\n",
    "xgb_predictions_df = pd.DataFrame({\n",
    "    'Actual Orders': y_test.reset_index(drop=True),  # Ensure alignment between actual and predicted\n",
    "    'Predicted Orders': y_pred_xgb\n",
    "})\n",
    "\n",
    "print(\"\\nXGBoost Predicted vs Actual Orders:\")\n",
    "print(xgb_predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f62cc-8baf-407c-a263-ce8aac1655c7",
   "metadata": {},
   "source": [
    "# 4. Route Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ab03d-9da3-4098-a527-23f8935cfc20",
   "metadata": {},
   "source": [
    "## 4.1. Clustering by parcel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f7680-c0ad-491c-a862-579c514dd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Product_volume'] = df['product_length_cm']* df['product_height_cm']* df['product_width_cm']\n",
    "df.dropna(subset=['Product_volume'], inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b9313-6271-4cb3-b845-2de44c2943a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parcels (desiered number of clusters)\n",
    "# Super small, small, meduim, big, Super big\n",
    "max_num_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a6c76-f1d1-4258-9964-124f1aedb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the parcels based on the size of the parcel\n",
    "kmeans = KMeans(n_clusters=max_num_size)\n",
    "# Reshape the input data to a 2D array\n",
    "kmeans.fit(df['Product_volume'].values.reshape(-1, 1))\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Product_volume'], np.zeros_like(df['Product_volume']), c=cluster_labels, cmap='viridis', s=50)\n",
    "plt.scatter(cluster_centers, np.zeros_like(cluster_centers), c='red', marker='X', s=300, label='Cluster Centers')\n",
    "plt.title('Clustering of Parcels based on Volumes')\n",
    "plt.xlabel('Volume')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Printing cluster centers\n",
    "print(\"Cluster Centers:\")\n",
    "for i, center in enumerate(cluster_centers):\n",
    "    print(f\"Cluster {i+1}: {center[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f05226-5be1-475b-8141-f34fbe24115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Centers (derived from result above)\n",
    "cluster_centers = {\n",
    "  'Cluster 1': 22953.805372673138,\n",
    "  'Cluster 2': 102682.53763440774,\n",
    "  'Cluster 3': 53206.30492355889,\n",
    "  'Cluster 4': 4724.973529788527,\n",
    "  'Cluster 5': 211177.80929095377\n",
    "}\n",
    "\n",
    "# Define categories based on cluster centers (tag the parcels with different sizes)\n",
    "categories = {\n",
    "    'super small': cluster_centers['Cluster 4'],\n",
    "    'small': cluster_centers['Cluster 1'],\n",
    "    'medium': cluster_centers['Cluster 3'],\n",
    "    'big': cluster_centers['Cluster 2'],\n",
    "    'super big': cluster_centers['Cluster 5']\n",
    "}\n",
    "\n",
    "# Assign parcels to categories based on cluster centers\n",
    "def assign_parcels_to_categories(cluster_centers, categories, product_volumes):\n",
    "    assigned_categories = []\n",
    "    for volume in product_volumes:\n",
    "        for category, threshold in categories.items():\n",
    "            if volume <= threshold:\n",
    "                assigned_categories.append((volume, category))\n",
    "                break\n",
    "    return assigned_categories\n",
    "\n",
    "# Assign parcels to categories\n",
    "assigned_categories = assign_parcels_to_categories(cluster_centers, categories, df['Product_volume'])\n",
    "\n",
    "# Print the assigned categories for each parcel volume\n",
    "# print(\"Assigned categories for each parcel volume:\")\n",
    "# for volume, category in assigned_categories:\n",
    "#    print(f\"Volume: {volume}, Category: {category}\")\n",
    "\n",
    "# A dictionary to store the count of parcels in each category\n",
    "category_counts = {category: 0 for category in categories.keys()}\n",
    "\n",
    "# Count the number of parcels in each category\n",
    "for _, category in assigned_categories:\n",
    "    category_counts[category] += 1\n",
    "\n",
    "# Print the number of parcels in each category\n",
    "print(\"Number of parcels in each category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c965863-eb1c-4b67-8486-7826ef4814ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parcels in each cluster (derived from result above)\n",
    "parcel_counts = {\n",
    "    'super small': 46606,\n",
    "    'small': 44636,\n",
    "    'medium': 15251,\n",
    "    'big': 5374,\n",
    "    \"super big\": 1252\n",
    "}\n",
    "\n",
    "# van capacity (maximum volume of parcels that van can load)\n",
    "van_capacity = 500000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d184c-fe86-4834-b4a3-2c879f21de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "loaded_parcels = {}\n",
    "remaining_capacity = van_capacity\n",
    "\n",
    "# Iterate through categories and allocate parcels proportionally\n",
    "for category, count in parcel_counts.items():\n",
    "    # Calculate the proportion of the van capacity for the current category\n",
    "    category_proportion = count / sum(parcel_counts.values())\n",
    "    # Calculate the volume of parcels to be loaded for this category\n",
    "    category_volume = van_capacity * category_proportion\n",
    "    # Update the loaded parcels with the rounded count\n",
    "    loaded_parcels[category] = round(category_volume / categories[category])\n",
    "    # Update the remaining capacity\n",
    "    remaining_capacity -= loaded_parcels[category] * categories[category]\n",
    "\n",
    "# Print the loaded parcels\n",
    "print(\"Optimal combination of parcels:\")\n",
    "for category, count in loaded_parcels.items():\n",
    "    print(f\"{count} {category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc886e3-fe56-4307-941f-c9043e8acccb",
   "metadata": {},
   "source": [
    "## 4.2. Clustering by parcel size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55232a22-5acf-4833-b4c8-be1649ed6de4",
   "metadata": {},
   "source": [
    "### 4.2.1. Data filtering & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e379280-2d50-47f0-80e9-c35bf5df0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for rows where order_delivered_carrier_date is '2017-05-05' and customer_city in 'sao paulo'\n",
    "route_optimisation = route_optimisation.dropna(subset=['order_delivered_carrier_date'])\n",
    "filtered_data = route_optimisation[\n",
    "    (route_optimisation['order_delivered_carrier_date'].str.startswith('2017-05-05')) &\n",
    "    (route_optimisation['customer_city'] == \"sao paulo\")]\n",
    "print(filtered_data)\n",
    "print(filtered_data.count())\n",
    "\n",
    "# Extract latitude and longitude coordinates from filtered_data\n",
    "latitudes = filtered_data['geolocation_lat']\n",
    "longitudes = filtered_data['geolocation_lng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f13eaf-a46f-4b79-bdf7-212bb6e0fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the geolocations\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(filtered_data['geolocation_lng'], filtered_data['geolocation_lat'], color='blue', label='Geolocations')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Geolocations with order_delivered_carrier_date_y = 2017-05-05')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7f190-70e0-4d1d-8935-90f9e2eaa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered at the mean latitude and longitude of the filtered data\n",
    "map = folium.Map(location=[filtered_data['geolocation_lat'].mean(), filtered_data['geolocation_lng'].mean()], zoom_start=10)\n",
    "\n",
    "# Add markers for each geolocation point\n",
    "for idx, row in filtered_data.iterrows():\n",
    "    folium.Marker([row['geolocation_lat'], row['geolocation_lng']]).add_to(map)\n",
    "\n",
    "# Display the map\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c6a5c-c860-40fa-bdc8-f6c27ea746e8",
   "metadata": {},
   "source": [
    "## 4.2.2. Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece5d51-e35b-4a48-a8e6-2880d5cbb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of drivers (desired number of clusters)\n",
    "max_num_drivers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40068d7d-e894-4f37-a643-74d4ae36abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine latitudes and longitudes into a 2D array\n",
    "locations = np.column_stack((latitudes, longitudes))\n",
    "\n",
    "# Cluster the locations based on the number of drivers\n",
    "kmeans = KMeans(n_clusters=max_num_drivers)\n",
    "kmeans.fit(locations)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(max_num_drivers):\n",
    "    # Plot points belonging to cluster i\n",
    "    plt.scatter(locations[labels == i, 1], locations[labels == i, 0], label=f'Cluster {i+1}')\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Clustered Geolocations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9eebce-14e8-4328-a8df-e054f2cfa25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered at the mean latitude and longitude of the cluster centers\n",
    "clustered_map = folium.Map(location=[cluster_centers[:, 0].mean(), cluster_centers[:, 1].mean()], zoom_start=10)\n",
    "\n",
    "# Generate random colors for each cluster\n",
    "colors = ['#' + ''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(max_num_drivers)]\n",
    "\n",
    "# Add markers for each point, color-coded by cluster\n",
    "for i in range(max_num_drivers):\n",
    "    cluster_points = locations[labels == i]\n",
    "    for point in cluster_points:\n",
    "        folium.CircleMarker(location=[point[0], point[1]], radius=5, color=colors[i], fill=True, fill_color=colors[i], fill_opacity=0.7).add_to(clustered_map)\n",
    "\n",
    "# Display the map\n",
    "clustered_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351649d-5ab8-478b-90dd-2611150a84ba",
   "metadata": {},
   "source": [
    "## 4.2.3. Evaluation of clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06701b-13da-405d-ae71-88a9a7bb3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Range of clusters to evaluate\n",
    "min_clusters = 1\n",
    "max_clusters = max_num_drivers\n",
    "\n",
    "# Evaluation for each cluster\n",
    "avg_distances = []\n",
    "avg_num_points = []\n",
    "avg_total_distances = []\n",
    "\n",
    "for num_clusters in range(min_clusters, max_clusters + 1):\n",
    "    # Step 2: Cluster the locations based on the number of clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(locations)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Calculate average distance for each cluster\n",
    "    avg_distance = 0\n",
    "    for i in range(num_clusters):\n",
    "        distances = np.linalg.norm(locations[labels == i] - cluster_centers[i], axis=1)\n",
    "        avg_distance += np.mean(distances)\n",
    "    \n",
    "    avg_distances.append(avg_distance / num_clusters)  # Divide by num_clusters to get average\n",
    "\n",
    "    # Calculate average number of points for each cluster\n",
    "    avg_num_points.append(len(locations) / num_clusters)\n",
    "\n",
    "    # Calculate average total distance for each cluster\n",
    "    total_distances = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        distances = np.linalg.norm(locations[labels == i] - cluster_centers[i], axis=1)\n",
    "        total_distances[i] = np.sum(distances)\n",
    "    avg_total_distances.append(np.mean(total_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e0c1e-83c3-49df-a73e-79a7c5085768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize average distances for each number of clusters\n",
    "plt.plot(range(min_clusters, max_clusters + 1), avg_distances, marker='o', color=\"green\")\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Average Distance')\n",
    "plt.title('Evaluation of Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47474f-ba12-4703-b467-698758dae748",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(min_clusters, max_clusters + 1), avg_num_points, marker='o', color='blue')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Average Number of Points')\n",
    "plt.title('Average Number of Points in Each Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f47127-fb6c-4e5a-83f0-ca5e7faa6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(min_clusters, max_clusters + 1), avg_total_distances, marker='x', color='orange')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Average Total Distance')\n",
    "plt.title('Average Total Distance in Each Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad837f-8634-4a1a-aea1-486f3811acd0",
   "metadata": {},
   "source": [
    "## 4.2.4. Optimal number of clusters based on evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131b084-1ddf-4ef1-bd5a-2b69d9f20eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal number of drivers (desired number of clusters)\n",
    "optimal_drivers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7bd0a-4e02-40d4-b870-f9e62584ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered at the mean latitude and longitude of the cluster centers\n",
    "optimal_map = folium.Map(location=[cluster_centers[:, 0].mean(), cluster_centers[:, 1].mean()], zoom_start=10)\n",
    "\n",
    "# Generate random colors for each cluster\n",
    "colors = ['#' + ''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(optimal_drivers)]\n",
    "\n",
    "# Add markers for each point, color-coded by cluster\n",
    "for i in range(optimal_drivers):\n",
    "    cluster_points = locations[labels == i]\n",
    "    for point in cluster_points:\n",
    "        folium.CircleMarker(location=[point[0], point[1]], radius=5, color=colors[i], fill=True, fill_color=colors[i], fill_opacity=0.7).add_to(optimal_map)\n",
    "\n",
    "# Display the map\n",
    "optimal_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58cae25-3e1b-4724-90cc-8032ff97d7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
